{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001F5B7BB2600>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 796, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1535, in enumerate\n",
      "    def enumerate():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m query = \u001b[33m'\u001b[39m\u001b[33mSELECT * FROM migration.sales_data --LIMIT 100;\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Use Pandas to read the SQL query into a DataFrame (57.8 sec)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m sales_data_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Print dataframe\u001b[39;00m\n\u001b[32m     26\u001b[39m sales_data_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\pandas\\io\\sql.py:736\u001b[39m, in \u001b[36mread_sql\u001b[39m\u001b[34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[39m\n\u001b[32m    726\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql.read_table(\n\u001b[32m    727\u001b[39m         sql,\n\u001b[32m    728\u001b[39m         index_col=index_col,\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m         dtype_backend=dtype_backend,\n\u001b[32m    734\u001b[39m     )\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\pandas\\io\\sql.py:1866\u001b[39m, in \u001b[36mSQLDatabase.read_query\u001b[39m\u001b[34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[39m\n\u001b[32m   1864\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1865\u001b[39m     data = result.fetchall()\n\u001b[32m-> \u001b[39m\u001b[32m1866\u001b[39m     frame = \u001b[43m_wrap_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1875\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m frame\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\pandas\\io\\sql.py:206\u001b[39m, in \u001b[36m_wrap_result\u001b[39m\u001b[34m(data, columns, index_col, coerce_float, parse_dates, dtype, dtype_backend)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_result\u001b[39m(\n\u001b[32m    197\u001b[39m     data,\n\u001b[32m    198\u001b[39m     columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    204\u001b[39m ):\n\u001b[32m    205\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrap result set of a SQLAlchemy query in a DataFrame.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     frame = \u001b[43m_convert_arrays_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype:\n\u001b[32m    209\u001b[39m         frame = frame.astype(dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\pandas\\io\\sql.py:169\u001b[39m, in \u001b[36m_convert_arrays_to_dataframe\u001b[39m\u001b[34m(data, columns, coerce_float, dtype_backend)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_arrays_to_dataframe\u001b[39m(\n\u001b[32m    163\u001b[39m     data,\n\u001b[32m    164\u001b[39m     columns,\n\u001b[32m    165\u001b[39m     coerce_float: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    166\u001b[39m     dtype_backend: DtypeBackend | Literal[\u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    167\u001b[39m ) -> DataFrame:\n\u001b[32m    168\u001b[39m     content = lib.to_object_array_tuples(data)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     arrays = \u001b[43mconvert_object_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_backend == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    176\u001b[39m         pa = import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1071\u001b[39m, in \u001b[36mconvert_object_array\u001b[39m\u001b[34m(content, dtype, dtype_backend, coerce_float)\u001b[39m\n\u001b[32m   1067\u001b[39m             arr = maybe_cast_to_datetime(arr, dtype)\n\u001b[32m   1069\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m arrays = [\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m content]\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arrays\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\db_to_db_python\\project_venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:1043\u001b[39m, in \u001b[36mconvert_object_array.<locals>.convert\u001b[39m\u001b[34m(arr)\u001b[39m\n\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m# Notes on cases that get here 2023-02-15\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;66;03m# 1) we DO get here when arr is all Timestamps and dtype=None\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;66;03m# 2) disabling this doesn't break the world, so this must be\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m#    getting caught at a higher level\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# 3) passing convert_non_numeric to maybe_convert_objects get this right\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# 4) convert_non_numeric?\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arr.dtype == \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m   1044\u001b[39m         \u001b[38;5;66;03m# i.e. maybe_convert_objects didn't convert\u001b[39;00m\n\u001b[32m   1045\u001b[39m         convert_to_nullable_dtype = dtype_backend != \u001b[33m\"\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1046\u001b[39m         arr = maybe_infer_to_datetimelike(arr, convert_to_nullable_dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Connect to postgresql db using sqlalchemy library and read data from it to pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "postgresql_user = 'postgres'\n",
    "postgresql_password = '1'\n",
    "postgresql_host = 'localhost'\n",
    "postgresql_port = '5433'\n",
    "postgresql_dbname = 'sales'\n",
    "\n",
    "#database_uri = 'postgresql+psycopg2://user:password@host:port/dbname'\n",
    "\n",
    "database_uri = f'postgresql+psycopg2://{postgresql_user}:{postgresql_password}@{postgresql_host}:{postgresql_port}/{postgresql_dbname}'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(database_uri)\n",
    "\n",
    "# Define the sql query\n",
    "query = 'SELECT * FROM migration.sales_data --LIMIT 100;'\n",
    "\n",
    "# Use Pandas to read the SQL query into a DataFrame (57.8 sec)\n",
    "sales_data_df = pd.read_sql(query, engine) \n",
    "\n",
    "# Print dataframe\n",
    "sales_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new field \"Shipping duration\" to calculate the difference between ship_date and order_date\n",
    "\n",
    "## Create a new dataframe that will store the new field\n",
    "updated_sales_data_df = sales_data_df.copy(deep=True)\n",
    "\n",
    "## Convert the order_date and ship_date columns to datetime\n",
    "updated_sales_data_df['order_date'] = pd.to_datetime(updated_sales_data_df['order_date'])\n",
    "updated_sales_data_df['ship_date'] = pd.to_datetime(updated_sales_data_df['ship_date'])\n",
    "\n",
    "## Calculate the difference between ship_date and order_date\n",
    "updated_sales_data_df['shipping_duration'] = (updated_sales_data_df['ship_date'] - updated_sales_data_df['order_date']).dt.days\n",
    "\n",
    "## Print the updated dataframe (see the last column)\n",
    "updated_sales_data_df\n",
    "\n",
    "## Send back updated dataframe to Postgres to the table `updated_sales_data` (9 mins)\n",
    "#updated_sales_data_df.to_sql('updated_sales_data', con=engine, schema='migration', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size=10000 and max_workers=4 - 4 min 30 sec\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to write chunks to SQL Server\n",
    "def write_to_sql(chunk):\n",
    "    try:\n",
    "        chunk.to_sql(name='updated_sales_data', con=engine, schema='migration', if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Split DataFrame into chunks (number of rows)\n",
    "chunk_size = 10000\n",
    "chunks = [updated_sales_data_df[i:i+chunk_size] for i in range(0, updated_sales_data_df.shape[0], chunk_size)] \n",
    "\n",
    "# Use ThreadPoolExecutor to write chunks in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:  # Adjust max_workers as needed\n",
    "    # Submit all the tasks and keep track of the futures\n",
    "    futures = [executor.submit(write_to_sql, chunk) for chunk in chunks]\n",
    "\n",
    "    # Using as_completed to ensure all futures are done and to handle exceptions\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            # result() will re-raise any exception caught during execution.\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            # Handle or log the exception if needed\n",
    "            print(f\"An error occurred during database operation: {e}\")\n",
    "\n",
    "# At this point, all futures have completed, and all chunks have been processed\n",
    "print(\"All tasks are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to snowflake  using snowflake connector and read data from it to pandas dataframe\n",
    "\n",
    "import snowflake.connector \n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "          user='user',\n",
    "          password='password',\n",
    "          account='snowflake_account',\n",
    "          warehouse='WH_SUPERSTORE',\n",
    "          database='RAW',\n",
    "          schema='SUPERSTORE')\n",
    "\n",
    "# Create a cursor object.\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute a statement that will generate a result set.\n",
    "sql = \"select * from orders limit 100\"\n",
    "cur.execute(sql)\n",
    "\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "df = cur.fetch_pandas_all()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrate data from PostgreSQL database table to Snowflake using Pandas dataframe as intermediary \n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import snowflake.connector \n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "postgresql_user     = 'postgres'\n",
    "postgresql_password = '1'\n",
    "postgresql_host     = 'localhost'\n",
    "postgresql_port     = '5433'\n",
    "postgresql_dbname   = 'sales'\n",
    "\n",
    "# Create connection string for PostgreSQL\n",
    "database_uri = f'postgresql+psycopg2://{postgresql_user}:{postgresql_password}@{postgresql_host}:{postgresql_port}/{postgresql_dbname}'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(database_uri)\n",
    "\n",
    "# Define the sql query\n",
    "query = 'SELECT * FROM migration.sales_data -- LIMIT 3000000;'\n",
    "\n",
    "# Use Pandas to read the SQL query into a DataFrame\n",
    "sales_data_df = pd.read_sql(query, engine)\n",
    "print(\"# of rows in sales_data_df = \", len(sales_data_df))\n",
    "\n",
    "# Configure connection to Snowflake\n",
    "snowflake_conn = snowflake.connector.connect(\n",
    "          user='user',\n",
    "          password='password',\n",
    "          account='snowflake_account',\n",
    "          warehouse='WH_SUPERSTORE',\n",
    "          database='<your_name>_MIGRATION', #Put your db name here\n",
    "          schema='RAW')\n",
    "\n",
    "success, nchunks, nrows, _ = write_pandas(snowflake_conn, sales_data_df, 'SALES_DATA4',  auto_create_table=True)\n",
    "print(f'success = {success}, nchunks = {nchunks}, nrows = {nrows}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
